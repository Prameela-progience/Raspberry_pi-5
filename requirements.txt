ppe_surveillance_ai/
├── models/
│   └── yolov8n.pt
├── src/
│   ├── main_application.py                  # App Controller & Navigation
│   ├── video_stream_worker.py               # Threading & Signal Logic
│   ├── object_detection_inference_engine.py # Pure AI Logic (YOLO)
│   ├── Gui_components.py                    # Visual Styles & Widgets
│   └── Gui_screens.py                       # Layouts (Home, Monitoring)
├── requirements.txt
└── README.md






''' 
FILE:object_detection_inference_engine.py
'''
import sys
import os
import cv2
import time
import numpy as np
from ultralytics import YOLO
from PyQt6.QtWidgets import (
    QApplication, QMainWindow, QStackedWidget, QWidget,
    QVBoxLayout, QHBoxLayout, QLabel, QPushButton,
    QFrame, QLineEdit, QTextEdit, QScrollArea,
    QProgressBar, QComboBox, QFileDialog
)
from PyQt6.QtCore import Qt, QThread, pyqtSignal
from PyQt6.QtGui import QImage, QPixmap, QColor
from PyQt6.QtCore import QTimer

# =========================================================
# CORE LOGIC: MEDIA PROCESSING THREAD
# =========================================================
class MediaProcessorThread(QThread):
    signal_raw_stream = pyqtSignal(QPixmap)
    signal_sampled_audit_stream = pyqtSignal(QPixmap)
    signal_inference_result_stream = pyqtSignal(QPixmap)
    signal_error = pyqtSignal(str) # Added to alert UI of camera failures

    def __init__(self, media_source=0, inference_target_fps=30):
        super().__init__()
        # Use a string for file paths or an integer for cameras
        self.media_source = media_source
        self.inference_target_fps = inference_target_fps
        
        # Use an Atomic-style flag for thread control
        self._is_running = True 
        self.is_inference_enabled = False
        
        # Load model once during initialization to avoid lag later
        

        MODEL_PATH = os.path.join(os.path.dirname(__file__), "best.pt")

        if os.path.exists(MODEL_PATH):
            #print("Loading custom YOLO model:", MODEL_PATH)
            self.inference_engine = YOLO(MODEL_PATH)
        else:
            #print("Custom model not found. Loading default yolov8n.pt")
            self.inference_engine = YOLO("yolov8n.pt")

    def update_inference_fps(self, new_fps):
        self.inference_target_fps = int(new_fps)

    def toggle_inference_engine(self, activation_state: bool):
        self.is_inference_enabled = activation_state

    def run(self):
        cap = cv2.VideoCapture(self.media_source)
        if not cap.isOpened():
            print(f"[ERROR] Could not open source: {self.media_source}")
            return

        fps_source = cap.get(cv2.CAP_PROP_FPS)
        if fps_source <= 0: fps_source = 30.0 
        
        # Track the last FPS to detect dynamic changes
        last_printed_fps = None
        frame_counter = 0

        while self._is_running:
            loop_start = time.time()
            ret, frame = cap.read()
            if not ret: break
            
            frame_counter += 1
            
            # 1. ALWAYS emit Raw Preview (Box 1)
            raw_pixmap = self.convert_cv_to_qpixmap(frame)
            self.signal_raw_stream.emit(raw_pixmap)

            # Recalculate interval dynamically
            current_target = self.inference_target_fps
            current_sampling_interval = max(1, int(fps_source // current_target))
            
            # --- DYNAMIC DEBUG PRINT LOGIC ---
            # This triggers on the first run AND every time the dropdown is changed
            if current_target != last_printed_fps:
                if isinstance(self.media_source, str):
                    mode_label = "RECORDED MODE"
                elif current_target == 1:
                    mode_label = "VISION MODE"
                else:
                    mode_label = "VIDEO MODE"

                print(f"\n" + "-"*40)
                print(f" [UPDATE] {mode_label} ACTIVE")
                print(f" Source FPS  : {fps_source}")
                print(f" Selected FPS: {current_target}")
                print(f" Interval    : Every {current_sampling_interval} frame(s)")
                print("-"*40)
                
                last_printed_fps = current_target
            # ---------------------------------

            # 2. Sampling Logic (Box 2 and Box 3)
            if frame_counter % current_sampling_interval == 0:
                self.signal_sampled_audit_stream.emit(raw_pixmap)
                
                if self.is_inference_enabled:
                    results = self.inference_engine.predict(frame, conf=0.4, verbose=False)
                    annotated_frame = results[0].plot()
                    result_pixmap = self.convert_cv_to_qpixmap(annotated_frame)
                    self.signal_inference_result_stream.emit(result_pixmap)
            
            if frame_counter >= fps_source:
                frame_counter = 0

            # Timing Sync
            elapsed = time.time() - loop_start
            sleep_time = max(1, int((1.0 / fps_source - elapsed) * 1000))
            self.msleep(sleep_time)

        cap.release()
    

    def convert_cv_to_qpixmap(self, cv_img):
        """Converts BGR OpenCV images to RGB QPixmap."""
        rgb_image = cv2.cvtColor(cv_img, cv2.COLOR_BGR2RGB)
        h, w, ch = rgb_image.shape
        bytes_per_line = ch * w
        qt_image = QImage(rgb_image.data, w, h, bytes_per_line, QImage.Format.Format_RGB888)
        return QPixmap.fromImage(qt_image).copy() # .copy() ensures memory safety

    def terminate_thread(self):
        """Safely stops the loop, allowing the run() method to finish naturally."""
        self._is_running = False
        self.wait() # Wait for the thread to actually finish
    


'''
class MediaProcessorThread(QThread):
    """
    Background thread dedicated to high-intensity media acquisition and AI inference.
    Running this in a separate thread prevents the GUI from freezing during processing.
    """
    signal_raw_stream = pyqtSignal(QPixmap)            # Emits every frame for real-time preview
    signal_sampled_audit_stream = pyqtSignal(QPixmap)   # Emits frames at the specific 'Target FPS' rate
    signal_inference_result_stream = pyqtSignal(QPixmap) # Emits frames containing YOLO detection boxes

    def __init__(self, media_source=0, inference_target_fps=30):
        """
        Initializes the processor engine.
        :param media_source: 0 for webcam, or a string path for a video file.
        :param inference_target_fps: The rate at which the AI should sample and analyze frames.
        """
        super().__init__()
        self.media_source = media_source
        self.inference_target_fps = inference_target_fps
        self.thread_active_status = True
        self.is_inference_enabled = False  
        self.source_native_fps = 30  
        
        try:
            # Load the YOLO model; Nano (n) version is used for higher performance
            self.inference_engine = YOLO("yolov8n.pt") 
        except Exception as engine_error:
            print(f"Engine Load Error: {engine_error}")

    def update_inference_fps(self, new_fps):
        """Dynamically adjusts the sampling rate of the AI processing loop."""
        self.inference_target_fps = new_fps

    def toggle_inference_engine(self, activation_state: bool):
        """Allows toggling the AI detection ON/OFF without stopping the video stream."""
        self.is_inference_enabled = activation_state

    def run(self):
        """
        The main loop that captures frames, performs inference if enabled, 
        and calculates timing delays to maintain video synchronization.
        """
        capture_device = cv2.VideoCapture(self.media_source)
        
        # Determine the native FPS of the file to calculate skipping logic for unwanted Frames 
        if isinstance(self.media_source, str):
            source_fps = capture_device.get(cv2.CAP_PROP_FPS)
            if source_fps > 0:
                self.source_native_fps = int(source_fps)

        processed_frame_index = 0

        while self.thread_active_status:
            ret, cv_frame = capture_device.read()
            if not ret:
                break
            
            processed_frame_index += 1
            current_pixmap = self.convert_cv_to_qpixmap(cv_frame)
            
            # Emit raw stream for the 'System Feed' window
            self.signal_raw_stream.emit(current_pixmap)

            # Sampling Logic: Only process frames if they hit the target FPS interval
            sampling_interval = max(1, self.source_native_fps // self.inference_target_fps)
            
            if processed_frame_index % sampling_interval == 0:
                self.signal_sampled_audit_stream.emit(current_pixmap)
                
                # If analytics are ON, pass frame through YOLO
                if self.is_inference_enabled:
                    inference_results = self.inference_engine.predict(cv_frame, conf=0.4, verbose=False)
                    annotated_cv_frame = inference_results[0].plot() # Draws boxes/labels
                    result_pixmap = self.convert_cv_to_qpixmap(annotated_cv_frame)
                    self.signal_inference_result_stream.emit(result_pixmap)
            
            if processed_frame_index >= self.source_native_fps:
                processed_frame_index = 0

            # Sleep to match file FPS; if webcam, use a standard small delay
            if isinstance(self.media_source, str):
                time.sleep(1 / self.source_native_fps)
            else:
                time.sleep(0.01)

        capture_device.release()

    def convert_cv_to_qpixmap(self, opencv_frame):
        """Converts BGR NumPy arrays from OpenCV to RGB QPixmap for PyQt6 display."""
        rgb_data = cv2.cvtColor(opencv_frame, cv2.COLOR_BGR2RGB)
        h, w, ch = rgb_data.shape
        bytes_per_line = ch * w
        q_img = QImage(rgb_data.data, w, h, bytes_per_line, QImage.Format.Format_RGB888)
        return QPixmap.fromImage(q_img)

    def terminate_thread(self):
        """Gracefully stops the infinite processing loop."""
        self.thread_active_status = False

    '''


'''
def run(self):
        """Main execution loop using a controlled while-loop instead of QTimer."""
        cap = cv2.VideoCapture(self.media_source)
        
        if not cap.isOpened():
            self.signal_error.emit(f"Failed to open source: {self.media_source}")
            return

        # Determine timing based on source capabilities
        fps_source = cap.get(cv2.CAP_PROP_FPS)
        if fps_source <= 0: fps_source = 30 
        
        # --- DEBUG PRINT BLOCK START ---
        # Identify the mode based on the media_source type and target_fps
        if isinstance(self.media_source, str):
            mode_label = "RECORDED MODE"
        elif self.inference_target_fps == 1:
            mode_label = "VISION MODE"
        else:
            mode_label = "VIDEO MODE (LIVE)"

        print(f"\n{'='*40}")
        print(f" DEBUG: {mode_label}")
        print(f" Source: {self.media_source}")
        print(f" Default Source FPS: {fps_source if fps_source > 0 else 'Variable/Undefined'}")
        print(f" Selected Target FPS: {self.inference_target_fps}")
        print(f"{'='*40}\n")

        # --- DEBUG PRINT BLOCK END ---

        frame_counter = 0

        while self._is_running:
            loop_start = time.time()
            ret, frame = cap.read()
            
            if not ret:
                break
            
            frame_counter += 1
            
            # 1. Process Raw Preview (Emits every frame)
            raw_pixmap = self.convert_cv_to_qpixmap(frame)
            self.signal_raw_stream.emit(raw_pixmap)

            # 2. Sampling Logic for Inference/Audit
            sampling_interval = max(1, int(fps_source // self.inference_target_fps))
            
            if frame_counter % sampling_interval == 0:
                self.signal_sampled_audit_stream.emit(raw_pixmap)
                
                if self.is_inference_enabled:
                    # Perform YOLO Inference
                    results = self.inference_engine.predict(frame, conf=0.4, verbose=False)
                    annotated_frame = results[0].plot()
                    result_pixmap = self.convert_cv_to_qpixmap(annotated_frame)
                    self.signal_inference_result_stream.emit(result_pixmap)
            
            if frame_counter >= fps_source:
                frame_counter = 0

            # 3. Synchronization: Maintain smooth playback speed
            elapsed = time.time() - loop_start
            sleep_time = max(1, int((1.0 / fps_source - elapsed) * 1000))
            self.msleep(sleep_time) # QThread friendly sleep

        # Clean cleanup on exit
        cap.release()
        cv2.destroyAllWindows()

'''

